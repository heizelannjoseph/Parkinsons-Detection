{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c610692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\n",
      "PAHAW_PUBLIC exists: True\n",
      "Processed dir: C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — imports and paths\n",
    "import os, math, time, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\")\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"PaHaW_dataset\"\n",
    "PAHAW_PUBLIC = DATA_ROOT / \"PaHaW_public\"\n",
    "META_FILE = DATA_ROOT / \"PaHaW_files\" / \"corpus_PaHaW.xlsx\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PAHAW_PUBLIC exists:\", PAHAW_PUBLIC.exists())\n",
    "print(\"Processed dir:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded — ready to run the sequence builder (Cell 2).\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — helper functions (run this BEFORE the big builder)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def load_svc(path):\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f if ln.strip()!='']\n",
    "    if len(lines) < 2:\n",
    "        return pd.DataFrame(columns=[\"x\",\"y\",\"time\",\"pen\",\"azim\",\"alt\",\"press\"])\n",
    "    # skip header if first line looks like a small count\n",
    "    start_idx = 1 if len(lines[0].split()) <= 2 else 0\n",
    "    for ln in lines[start_idx:]:\n",
    "        parts = ln.split()\n",
    "        if len(parts) < 7:\n",
    "            # drop leading index tokens like \"00:\" if present\n",
    "            if parts and parts[0].endswith(':'):\n",
    "                parts = parts[1:]\n",
    "        if len(parts) >= 7:\n",
    "            parts = parts[-7:]\n",
    "        if len(parts) != 7:\n",
    "            continue\n",
    "        try:\n",
    "            x, y, ts, pen, az, alt, pr = parts\n",
    "            rows.append([float(x), float(y), float(ts), float(pen), float(az), float(alt), float(pr)])\n",
    "        except Exception:\n",
    "            continue\n",
    "    df = pd.DataFrame(rows, columns=[\"x\",\"y\",\"time\",\"pen\",\"azim\",\"alt\",\"press\"])\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df, smoothing_window=7, smoothing_poly=2):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    df['y'] = df['y'] - df['y'].mean()\n",
    "    df['time_s'] = df['time'] - df['time'].iloc[0]\n",
    "    df['dt'] = df['time_s'].diff().fillna(1/1000).replace(0, 1/1000)\n",
    "    if len(df) >= smoothing_window:\n",
    "        df['x_s'] = savgol_filter(df['x'], smoothing_window, smoothing_poly)\n",
    "        df['y_s'] = savgol_filter(df['y'], smoothing_window, smoothing_poly)\n",
    "    else:\n",
    "        df['x_s'] = df['x']\n",
    "        df['y_s'] = df['y']\n",
    "    df['vx'] = df['x_s'].diff().fillna(0) / df['dt']\n",
    "    df['vy'] = df['y_s'].diff().fillna(0) / df['dt']\n",
    "    df['speed'] = np.sqrt(df['vx']**2 + df['vy']**2)\n",
    "    df['ax'] = df['vx'].diff().fillna(0) / df['dt']\n",
    "    df['ay'] = df['vy'].diff().fillna(0) / df['dt']\n",
    "    df['accel'] = np.sqrt(df['ax']**2 + df['ay']**2)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num = df['vx']*df['ay'] - df['vy']*df['ax']\n",
    "        den = (df['vx']**2 + df['vy']**2)**1.5\n",
    "        df['curvature'] = np.abs(num) / (den + 1e-12)\n",
    "    df['curvature'] = df['curvature'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def segment_by_pen(df):\n",
    "    pen = df['pen'].values\n",
    "    strokes = []\n",
    "    start = None\n",
    "    for i, p in enumerate(pen):\n",
    "        if p == 1 and start is None:\n",
    "            start = i\n",
    "        if p == 0 and start is not None:\n",
    "            strokes.append((start, i-1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        strokes.append((start, len(df)-1))\n",
    "    return strokes\n",
    "\n",
    "def split_stroke_by_speed_minima(df, s, e, prom=0.05, dist=8, min_points=6):\n",
    "    seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "    speed = seg['speed'].to_numpy()\n",
    "    inv = -speed\n",
    "    peaks, _ = find_peaks(inv, prominence=prom, distance=dist)\n",
    "    if len(peaks)==0:\n",
    "        intervals = [(s,e)]\n",
    "    else:\n",
    "        cuts = [s + int(p) for p in peaks]\n",
    "        intervals = []\n",
    "        prev = s\n",
    "        for c in cuts:\n",
    "            intervals.append((prev, c))\n",
    "            prev = c+1\n",
    "        if prev <= e:\n",
    "            intervals.append((prev, e))\n",
    "    intervals = [iv for iv in intervals if (iv[1]-iv[0]+1) >= min_points]\n",
    "    if len(intervals)==0:\n",
    "        intervals = [(s,e)]\n",
    "    return intervals\n",
    "\n",
    "def split_all_strokes(df, pen_strokes, prom=0.05, dist=8, min_points=6):\n",
    "    subs = []\n",
    "    for (s,e) in pen_strokes:\n",
    "        parts = split_stroke_by_speed_minima(df, s, e, prom=prom, dist=dist, min_points=min_points)\n",
    "        subs.extend(parts)\n",
    "    return subs\n",
    "\n",
    "# Beta & ellipse helpers\n",
    "def beta_velocity(t, A, t0, t1, a, b):\n",
    "    t = np.array(t)\n",
    "    v = np.zeros_like(t, dtype=float)\n",
    "    denom = (t1 - t0) if (t1 - t0) != 0 else 1e-8\n",
    "    u = (t - t0) / denom\n",
    "    mask = (u > 0) & (u < 1)\n",
    "    uu = u[mask]\n",
    "    with np.errstate(all='ignore'):\n",
    "        v_mask = A * (uu**a) * ((1 - uu)**b)\n",
    "    v[mask] = v_mask\n",
    "    return v\n",
    "\n",
    "def fit_beta_to_substroke(df, s, e, min_points=8):\n",
    "    seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "    if len(seg) < min_points:\n",
    "        return None\n",
    "    t = seg['time_s'].to_numpy()\n",
    "    speed = seg['speed'].to_numpy()\n",
    "    A0 = max(np.max(speed), 1e-3)\n",
    "    t0_0 = t[0]\n",
    "    t1_0 = t[-1]\n",
    "    p0 = [A0, t0_0, t1_0, 2.0, 2.0]\n",
    "    bounds = ([0.0, t[0]-0.5, t[-1]-1.0, 0.05, 0.05],\n",
    "              [A0*20+1.0, t[0]+0.5, t[-1]+0.5, 12.0, 12.0])\n",
    "    try:\n",
    "        popt, _ = curve_fit(beta_velocity, t, speed, p0=p0, bounds=bounds, maxfev=20000)\n",
    "        return {\"A\":float(popt[0]), \"t0\":float(popt[1]), \"t1\":float(popt[2]), \"a\":float(popt[3]), \"b\":float(popt[4])}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fit_ellipse_least_squares(xs, ys):\n",
    "    x = xs[:,None]; y = ys[:,None]\n",
    "    D = np.hstack([x*x, x*y, y*y, x, y, np.ones_like(x)])\n",
    "    try:\n",
    "        U, S, Vt = np.linalg.svd(D, full_matrices=False)\n",
    "        a = Vt.T[:,-1]\n",
    "        A, B, Cc, Dd, E, Ff = a.flatten()\n",
    "    except Exception:\n",
    "        return None\n",
    "    denom = B*B - 4*A*Cc\n",
    "    if np.isclose(denom, 0):\n",
    "        return None\n",
    "    x0 = (2*Cc*Dd - B*E) / denom\n",
    "    y0 = (2*A*E - B*Dd) / denom\n",
    "    up = 2*(A*E*E + Cc*Dd*Dd + Ff*B*B - 2*B*Dd*E - A*Cc*Ff)\n",
    "    tmp = (A - Cc)**2 + B*B\n",
    "    sqrt_tmp = np.sqrt(np.abs(tmp))\n",
    "    down1 = (denom) * ((Cc - A) + sqrt_tmp)\n",
    "    down2 = (denom) * ((Cc - A) - sqrt_tmp)\n",
    "    if down1 == 0 or down2 == 0:\n",
    "        return None\n",
    "    try:\n",
    "        a_len = np.sqrt(np.abs(up / down1))\n",
    "        b_len = np.sqrt(np.abs(up / down2))\n",
    "    except Exception:\n",
    "        return None\n",
    "    if b_len > a_len:\n",
    "        a_len, b_len = b_len, a_len\n",
    "    theta = 0.5 * np.arctan2(B, (A - Cc))\n",
    "    ecc = np.sqrt(max(0, 1 - (b_len**2)/(a_len**2))) if a_len!=0 else 0.0\n",
    "    return {\"xc\":float(x0), \"yc\":float(y0), \"a\":float(a_len), \"b\":float(b_len), \"angle_rad\":float(theta), \"ecc\":float(ecc)}\n",
    "\n",
    "print(\"Helper functions loaded — ready to run the sequence builder (Cell 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbdcd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total svc files: 597\n",
      "Resuming from partial file...\n",
      "Already processed: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:   8%|▊         | 50/597 [01:18<50:10,  5.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 25 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  13%|█▎        | 75/597 [02:05<15:21,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 50 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  17%|█▋        | 100/597 [02:50<14:19,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 75 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  21%|██        | 125/597 [03:34<20:21,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 100 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  25%|██▌       | 150/597 [04:30<08:10,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 125 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  29%|██▉       | 175/597 [05:09<10:30,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 150 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  34%|███▎      | 200/597 [05:43<07:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 175 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  38%|███▊      | 225/597 [06:26<10:37,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 200 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  42%|████▏     | 250/597 [06:58<06:53,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 225 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  46%|████▌     | 275/597 [07:46<08:27,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 250 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  50%|█████     | 300/597 [08:19<08:31,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 275 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  54%|█████▍    | 325/597 [08:54<05:29,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 300 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  59%|█████▊    | 350/597 [09:34<06:31,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 325 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  63%|██████▎   | 375/597 [10:27<05:16,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 350 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  67%|██████▋   | 400/597 [11:03<05:55,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 375 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  71%|███████   | 425/597 [11:41<03:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 400 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  75%|███████▌  | 450/597 [12:06<01:57,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 425 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  80%|███████▉  | 475/597 [12:45<02:41,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 450 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  84%|████████▍ | 500/597 [13:16<02:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 475 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  88%|████████▊ | 525/597 [13:40<01:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 500 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  92%|█████████▏| 550/597 [14:25<01:37,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 525 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files:  96%|█████████▋| 575/597 [15:07<00:52,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved partial progress after 550 new files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVC files: 100%|██████████| 597/597 [15:50<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished!\n",
      "Total files attempted: 597\n",
      "Total sequences: 597\n",
      "Failed: 0\n",
      "Saved to: C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\data\\processed\\all_sequences.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:716: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — generate sequences for ALL .svc files (BLSTM-ready)\n",
    "import json, gc, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm   # plain tqdm (no notebook mode)\n",
    "\n",
    "# --- Load metadata ---\n",
    "meta = pd.read_excel(META_FILE)\n",
    "meta['ID'] = meta['ID'].astype(str).str.zfill(5)\n",
    "meta_map = dict(zip(meta['ID'], meta['Disease']))  # \"PD\" or \"H\"\n",
    "\n",
    "# --- List files ---\n",
    "svc_paths = sorted(PAHAW_PUBLIC.rglob(\"*.svc\"))\n",
    "print(\"Total svc files:\", len(svc_paths))\n",
    "\n",
    "# --- Storage ---\n",
    "all_seqs = []     # list of (n_substrokes, feature_dim) arrays\n",
    "all_labels = []   # PD=1, H=0 per file\n",
    "all_meta = []     # store {\"path\",\"subject\"}\n",
    "failed = []\n",
    "\n",
    "# --- Resume feature ---\n",
    "partial_file = PROCESSED_DIR / \"all_sequences_partial.pkl\"\n",
    "\n",
    "if partial_file.exists():\n",
    "    print(\"Resuming from partial file...\")\n",
    "    with open(partial_file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    all_seqs = data.get(\"seqs\", [])\n",
    "    all_labels = data.get(\"labels\", [])\n",
    "    all_meta = data.get(\"meta\", [])\n",
    "    processed_paths = set(data.get(\"paths\", []))\n",
    "else:\n",
    "    processed_paths = set()\n",
    "\n",
    "print(\"Already processed:\", len(processed_paths))\n",
    "\n",
    "# --- Feature vector for a single substroke ---\n",
    "def compute_substroke_feature_vector(seg, df_local, s_abs, e_abs):\n",
    "    duration = seg['time_s'].iloc[-1] - seg['time_s'].iloc[0]\n",
    "    amp_x = seg['x_s'].max() - seg['x_s'].min()\n",
    "    amp_y = seg['y_s'].max() - seg['y_s'].min()\n",
    "    amplitude = math.sqrt(amp_x**2 + amp_y**2)\n",
    "    mean_speed = seg['speed'].mean()\n",
    "    mean_press = seg['press'].mean()\n",
    "\n",
    "    # beta & ellipse\n",
    "    beta = fit_beta_to_substroke(df_local, s_abs, e_abs)\n",
    "    ell  = fit_ellipse_least_squares(seg['x_s'].to_numpy(),\n",
    "                                     seg['y_s'].to_numpy())\n",
    "\n",
    "    beta_A = beta['A'] if isinstance(beta, dict) else np.nan\n",
    "    beta_a = beta['a'] if isinstance(beta, dict) else np.nan\n",
    "    beta_b = beta['b'] if isinstance(beta, dict) else np.nan\n",
    "\n",
    "    ell_a  = ell['a'] if isinstance(ell, dict) else np.nan\n",
    "    ell_b  = ell['b'] if isinstance(ell, dict) else np.nan\n",
    "    ell_ecc = ell['ecc'] if isinstance(ell, dict) else np.nan\n",
    "\n",
    "    # simple fuzzy: compare against global medians\n",
    "    f_speed_high = 1.0 if mean_speed > np.nanmedian(df_local['speed']) else 0.0\n",
    "    f_press_high = 1.0 if mean_press > np.nanmedian(df_local['press']) else 0.0\n",
    "    f_curv_high = 1.0 if seg['curvature'].mean() > np.nanmedian(df_local['curvature']) else 0.0\n",
    "\n",
    "    return np.array([\n",
    "        duration, amplitude, mean_speed, mean_press,\n",
    "        beta_A, beta_a, beta_b,\n",
    "        ell_a, ell_b, ell_ecc,\n",
    "        f_speed_high, f_press_high, f_curv_high\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "new_files = 0\n",
    "\n",
    "for p in tqdm(svc_paths, desc=\"Processing SVC files\"):\n",
    "    pstr = str(p)\n",
    "\n",
    "    # skip already processed\n",
    "    if pstr in processed_paths:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_local = load_svc(p)\n",
    "        if len(df_local) < 5:\n",
    "            failed.append({\"path\": pstr, \"reason\": \"too short\"})\n",
    "            processed_paths.add(pstr)\n",
    "            continue\n",
    "\n",
    "        df_local = preprocess_df(df_local)\n",
    "        pen_strokes = segment_by_pen(df_local)\n",
    "        subs = split_all_strokes(df_local, pen_strokes, prom=0.05, dist=8, min_points=6)\n",
    "\n",
    "        feats = []\n",
    "        for (s, e) in subs:\n",
    "            seg = df_local.iloc[s:e+1].reset_index(drop=True)\n",
    "            if len(seg) < 6:\n",
    "                continue\n",
    "            vec = compute_substroke_feature_vector(seg, df_local, s, e)\n",
    "            feats.append(vec)\n",
    "\n",
    "        if len(feats) == 0:\n",
    "            failed.append({\"path\": pstr, \"reason\": \"no substrokes\"})\n",
    "            processed_paths.add(pstr)\n",
    "            continue\n",
    "\n",
    "        seq_arr = np.vstack(feats)\n",
    "        all_seqs.append(seq_arr)\n",
    "\n",
    "        # label based on subject ID\n",
    "        subject = p.parent.name\n",
    "        label = 1 if meta_map.get(subject, \"H\") == \"PD\" else 0\n",
    "        all_labels.append(label)\n",
    "\n",
    "        all_meta.append({\"path\": pstr, \"subject\": subject})\n",
    "\n",
    "        processed_paths.add(pstr)\n",
    "        new_files += 1\n",
    "\n",
    "        # periodic saving every 25 files\n",
    "        if new_files % 25 == 0:\n",
    "            with open(partial_file, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"seqs\": all_seqs,\n",
    "                    \"labels\": all_labels,\n",
    "                    \"meta\": all_meta,\n",
    "                    \"paths\": list(processed_paths)\n",
    "                }, f)\n",
    "            print(f\"\\nSaved partial progress after {new_files} new files.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append({\"path\": pstr, \"reason\": str(e)})\n",
    "        processed_paths.add(pstr)\n",
    "        continue\n",
    "\n",
    "\n",
    "# --- Final save ---\n",
    "np.savez(PROCESSED_DIR / \"all_sequences.npz\",\n",
    "         sequences=all_seqs,\n",
    "         labels=all_labels,\n",
    "         meta=all_meta)\n",
    "\n",
    "with open(PROCESSED_DIR / \"all_sequences_failed.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"failed\": failed}, f)\n",
    "\n",
    "print(\"\\nFinished!\")\n",
    "print(\"Total files attempted:\", len(svc_paths))\n",
    "print(\"Total sequences:\", len(all_seqs))\n",
    "print(\"Failed:\", len(failed))\n",
    "print(\"Saved to:\", PROCESSED_DIR / \"all_sequences.npz\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
