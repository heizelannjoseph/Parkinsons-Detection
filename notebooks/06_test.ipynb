{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9bda67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\aibel\\OneDrive\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]\n",
      "OS: Windows-10-10.0.26100-SP0\n",
      "Working directory: c:\\Users\\aibel\\OneDrive\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\backups\\latest\n",
      "PROJECT_ROOT exists: True\n",
      "PROCESSED exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os, platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\aibel\\OneDrive\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\")\n",
    "PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "print(\"PROJECT_ROOT exists:\", PROJECT_ROOT.exists())\n",
    "print(\"PROCESSED exists:\", PROCESSED.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5454ee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model, scaler, and imputer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "model = load_model(MODEL_DIR / \"blstm_pahaw_model.h5\")\n",
    "\n",
    "with open(PROCESSED / \"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED / \"imputer.pkl\", \"rb\") as f:\n",
    "    imputer = pickle.load(f)\n",
    "\n",
    "print(\"✅ Model, scaler, and imputer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867984ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def load_svc(path):\n",
    "    rows = []\n",
    "    with open(path, 'r', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            parts = ln.strip().split()\n",
    "            if len(parts) >= 7:\n",
    "                parts = parts[-7:]\n",
    "                try:\n",
    "                    x, y, t, pen, az, alt, pr = parts\n",
    "                    rows.append([float(x), float(y), float(t), int(float(pen)),\n",
    "                                 float(az), float(alt), float(pr)])\n",
    "                except:\n",
    "                    continue\n",
    "    return pd.DataFrame(rows, columns=[\"x\",\"y\",\"time\",\"pen\",\"azim\",\"alt\",\"press\"])\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    df['y'] -= df['y'].mean()\n",
    "    df['time_s'] = df['time'] - df['time'].iloc[0]\n",
    "    df['dt'] = df['time_s'].diff().fillna(1/1000).replace(0, 1/1000)\n",
    "\n",
    "    if len(df) >= 7:\n",
    "        df['x_s'] = savgol_filter(df['x'], 7, 2)\n",
    "        df['y_s'] = savgol_filter(df['y'], 7, 2)\n",
    "    else:\n",
    "        df['x_s'] = df['x']\n",
    "        df['y_s'] = df['y']\n",
    "\n",
    "    df['vx'] = df['x_s'].diff().fillna(0) / df['dt']\n",
    "    df['vy'] = df['y_s'].diff().fillna(0) / df['dt']\n",
    "    df['speed'] = np.sqrt(df['vx']**2 + df['vy']**2)\n",
    "\n",
    "    df['ax'] = df['vx'].diff().fillna(0) / df['dt']\n",
    "    df['ay'] = df['vy'].diff().fillna(0) / df['dt']\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num = df['vx']*df['ay'] - df['vy']*df['ax']\n",
    "        den = (df['vx']**2 + df['vy']**2)**1.5\n",
    "        df['curvature'] = np.abs(num) / (den + 1e-12)\n",
    "\n",
    "    df['curvature'] = df['curvature'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def segment_by_pen(df):\n",
    "    strokes = []\n",
    "    start = None\n",
    "    for i,p in enumerate(df['pen']):\n",
    "        if p == 1 and start is None:\n",
    "            start = i\n",
    "        elif p == 0 and start is not None:\n",
    "            strokes.append((start, i-1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        strokes.append((start, len(df)-1))\n",
    "    return strokes\n",
    "\n",
    "def split_all_strokes(df, pen_strokes, prom=0.05, dist=8, min_points=6):\n",
    "    subs = []\n",
    "    for (s,e) in pen_strokes:\n",
    "        seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "        speed = seg['speed'].to_numpy()\n",
    "        peaks, _ = find_peaks(-speed, prominence=prom, distance=dist)\n",
    "        if len(peaks) == 0:\n",
    "            subs.append((s,e))\n",
    "        else:\n",
    "            prev = s\n",
    "            for p in peaks:\n",
    "                cut = s + int(p)\n",
    "                if cut-prev >= min_points:\n",
    "                    subs.append((prev, cut))\n",
    "                prev = cut+1\n",
    "            if e-prev >= min_points:\n",
    "                subs.append((prev, e))\n",
    "    return subs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_substroke_feature_vector(seg, df, s, e):\n",
    "    # Temporal & spatial\n",
    "    duration = seg['time_s'].iloc[-1] - seg['time_s'].iloc[0]\n",
    "\n",
    "    amp_x = seg['x_s'].max() - seg['x_s'].min()\n",
    "    amp_y = seg['y_s'].max() - seg['y_s'].min()\n",
    "    amp = np.sqrt(amp_x**2 + amp_y**2)\n",
    "\n",
    "    # Kinematic\n",
    "    mean_speed = seg['speed'].mean()\n",
    "    mean_press = seg['press'].mean()\n",
    "    mean_curvature = seg['curvature'].mean()  \n",
    "\n",
    "    # Beta \n",
    "    beta_A = mean_speed\n",
    "    beta_a = 2.0\n",
    "    beta_b = 2.0\n",
    "\n",
    "    # Ellipse \n",
    "    ell_a = amp\n",
    "    ell_b = amp / 2 if amp != 0 else 0.0\n",
    "    ell_ecc = np.sqrt(1 - (ell_b**2)/(ell_a**2)) if ell_a != 0 else 0.0\n",
    "\n",
    "    # Fuzzy perceptual flags\n",
    "    f_speed_high = 1.0 if mean_speed > np.median(df['speed']) else 0.0\n",
    "    f_press_high = 1.0 if mean_press > np.median(df['press']) else 0.0\n",
    "    f_curv_high  = 1.0 if mean_curvature > np.median(df['curvature']) else 0.0\n",
    "\n",
    "    return np.array([\n",
    "        duration,           # 1\n",
    "        amp,                # 2\n",
    "        mean_speed,         # 3\n",
    "        mean_press,         # 4\n",
    "        mean_curvature,     # 5  ← FIX\n",
    "        beta_A,             # 6\n",
    "        beta_a,             # 7\n",
    "        beta_b,             # 8\n",
    "        ell_a,              # 9\n",
    "        ell_b,              # 10\n",
    "        ell_ecc,            # 11\n",
    "        f_speed_high,       # 12\n",
    "        f_press_high,       # 13\n",
    "        f_curv_high         # 14\n",
    "    ], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51747301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 files for subject 00040\n",
      "\n",
      "Processing: 00040__1_1.svc\n",
      "\n",
      "Processing: 00040__2_1.svc\n",
      "\n",
      "Processing: 00040__3_1.svc\n",
      "\n",
      "Processing: 00040__4_1.svc\n",
      "\n",
      "Processing: 00040__5_1.svc\n",
      "\n",
      "Processing: 00040__6_1.svc\n",
      "\n",
      "Processing: 00040__7_1.svc\n",
      "\n",
      "Processing: 00040__8_1.svc\n",
      "\n",
      "==============================\n",
      "FINAL SUBJECT-LEVEL RESULT\n",
      "==============================\n",
      "Subject: 00040\n",
      "File-wise predictions: ['PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD']\n",
      "Vote counts: {'PD': 8}\n",
      "Mean PD probability: 0.828\n",
      "Final Diagnosis: Parkinson's\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SUBJECT-LEVEL PREDICTION\n",
    "\n",
    "SUBJECT_ID = \"00040\"   \n",
    "SUBJECT_DIR = PROJECT_ROOT / \"data\" / \"PaHaW_dataset\" / \"PaHaW_public\" / SUBJECT_ID\n",
    "\n",
    "svc_files = sorted(SUBJECT_DIR.glob(\"*.svc\"))\n",
    "print(f\"Found {len(svc_files)} files for subject {SUBJECT_ID}\")\n",
    "\n",
    "pd_scores = []\n",
    "file_preds = []\n",
    "\n",
    "for svc in svc_files:\n",
    "    print(\"\\nProcessing:\", svc.name)\n",
    "\n",
    "    df = load_svc(svc)\n",
    "    df = preprocess_df(df)\n",
    "    pen_strokes = segment_by_pen(df)\n",
    "    subs = split_all_strokes(df, pen_strokes, prom=0.05, dist=8, min_points=6)\n",
    "\n",
    "    features = []\n",
    "    for (s, e) in subs:\n",
    "        seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "        if len(seg) < 6:\n",
    "            continue\n",
    "        vec = compute_substroke_feature_vector(seg, df, s, e)\n",
    "        features.append(vec)\n",
    "\n",
    "    if len(features) == 0:\n",
    "        print(\"⚠️ Skipped (no valid substrokes)\")\n",
    "        continue\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    X[np.isinf(X)] = np.nan\n",
    "    X = imputer.transform(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    CAP = 300\n",
    "    feat_dim = X.shape[1]\n",
    "\n",
    "    if X.shape[0] > CAP:\n",
    "        X = X[:CAP]\n",
    "    else:\n",
    "        pad = CAP - X.shape[0]\n",
    "        X = np.vstack([X, np.zeros((pad, feat_dim))])\n",
    "\n",
    "    X = X.reshape(1, CAP, feat_dim)\n",
    "\n",
    "    prob = model.predict(X, verbose=0)[0][0]\n",
    "    label = \"PD\" if prob >= 0.5 else \"Healthy\"\n",
    "\n",
    "    pd_scores.append(prob)\n",
    "    file_preds.append(label)\n",
    "\n",
    "# FINAL SUBJECT-LEVEL DECISION - MAJORITY VOTING + MEAN SCORE\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "vote_counts = Counter(file_preds)\n",
    "majority_label = vote_counts.most_common(1)[0][0]\n",
    "\n",
    "mean_pd = float(np.mean(pd_scores))\n",
    "\n",
    "if vote_counts[\"PD\"] == vote_counts[\"Healthy\"]:\n",
    "    final_label = \"Parkinson's\" if mean_pd >= 0.5 else \"Healthy\"\n",
    "else:\n",
    "    final_label = \"Parkinson's\" if majority_label == \"PD\" else \"Healthy\"\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"FINAL SUBJECT-LEVEL RESULT\")\n",
    "print(\"==============================\")\n",
    "print(\"Subject:\", SUBJECT_ID)\n",
    "print(\"File-wise predictions:\", file_preds)\n",
    "print(\"Vote counts:\", dict(vote_counts))\n",
    "print(\"Mean PD probability:\", round(mean_pd, 3))\n",
    "print(\"Final Diagnosis:\", final_label)\n",
    "print(\"==============================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
