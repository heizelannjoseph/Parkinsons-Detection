{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c610692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\n",
      "PAHAW_PUBLIC exists: True\n",
      "Processed dir: C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — imports and paths\n",
    "import os, math, time, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\aibel\\Desktop\\Heizel Ann Joseph\\Parkinsons Disease\")\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"PaHaW_dataset\"\n",
    "PAHAW_PUBLIC = DATA_ROOT / \"PaHaW_public\"\n",
    "META_FILE = DATA_ROOT / \"PaHaW_files\" / \"corpus_PaHaW.xlsx\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PAHAW_PUBLIC exists:\", PAHAW_PUBLIC.exists())\n",
    "print(\"Processed dir:\", PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded — ready to run the sequence builder (Cell 2).\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — helper functions (run this BEFORE the big builder)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def load_svc(path):\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f if ln.strip()!='']\n",
    "    if len(lines) < 2:\n",
    "        return pd.DataFrame(columns=[\"x\",\"y\",\"time\",\"pen\",\"azim\",\"alt\",\"press\"])\n",
    "    # skip header if first line looks like a small count\n",
    "    start_idx = 1 if len(lines[0].split()) <= 2 else 0\n",
    "    for ln in lines[start_idx:]:\n",
    "        parts = ln.split()\n",
    "        if len(parts) < 7:\n",
    "            # drop leading index tokens like \"00:\" if present\n",
    "            if parts and parts[0].endswith(':'):\n",
    "                parts = parts[1:]\n",
    "        if len(parts) >= 7:\n",
    "            parts = parts[-7:]\n",
    "        if len(parts) != 7:\n",
    "            continue\n",
    "        try:\n",
    "            x, y, ts, pen, az, alt, pr = parts\n",
    "            rows.append([float(x), float(y), float(ts), float(pen), float(az), float(alt), float(pr)])\n",
    "        except Exception:\n",
    "            continue\n",
    "    df = pd.DataFrame(rows, columns=[\"x\",\"y\",\"time\",\"pen\",\"azim\",\"alt\",\"press\"])\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df, smoothing_window=7, smoothing_poly=2):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    df['y'] = df['y'] - df['y'].mean()\n",
    "    df['time_s'] = df['time'] - df['time'].iloc[0]\n",
    "    df['dt'] = df['time_s'].diff().fillna(1/1000).replace(0, 1/1000)\n",
    "    if len(df) >= smoothing_window:\n",
    "        df['x_s'] = savgol_filter(df['x'], smoothing_window, smoothing_poly)\n",
    "        df['y_s'] = savgol_filter(df['y'], smoothing_window, smoothing_poly)\n",
    "    else:\n",
    "        df['x_s'] = df['x']\n",
    "        df['y_s'] = df['y']\n",
    "    df['vx'] = df['x_s'].diff().fillna(0) / df['dt']\n",
    "    df['vy'] = df['y_s'].diff().fillna(0) / df['dt']\n",
    "    df['speed'] = np.sqrt(df['vx']**2 + df['vy']**2)\n",
    "    df['ax'] = df['vx'].diff().fillna(0) / df['dt']\n",
    "    df['ay'] = df['vy'].diff().fillna(0) / df['dt']\n",
    "    df['accel'] = np.sqrt(df['ax']**2 + df['ay']**2)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num = df['vx']*df['ay'] - df['vy']*df['ax']\n",
    "        den = (df['vx']**2 + df['vy']**2)**1.5\n",
    "        df['curvature'] = np.abs(num) / (den + 1e-12)\n",
    "    df['curvature'] = df['curvature'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def segment_by_pen(df):\n",
    "    pen = df['pen'].values\n",
    "    strokes = []\n",
    "    start = None\n",
    "    for i, p in enumerate(pen):\n",
    "        if p == 1 and start is None:\n",
    "            start = i\n",
    "        if p == 0 and start is not None:\n",
    "            strokes.append((start, i-1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        strokes.append((start, len(df)-1))\n",
    "    return strokes\n",
    "\n",
    "def split_stroke_by_speed_minima(df, s, e, prom=0.05, dist=8, min_points=6):\n",
    "    seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "    speed = seg['speed'].to_numpy()\n",
    "    inv = -speed\n",
    "    peaks, _ = find_peaks(inv, prominence=prom, distance=dist)\n",
    "    if len(peaks)==0:\n",
    "        intervals = [(s,e)]\n",
    "    else:\n",
    "        cuts = [s + int(p) for p in peaks]\n",
    "        intervals = []\n",
    "        prev = s\n",
    "        for c in cuts:\n",
    "            intervals.append((prev, c))\n",
    "            prev = c+1\n",
    "        if prev <= e:\n",
    "            intervals.append((prev, e))\n",
    "    intervals = [iv for iv in intervals if (iv[1]-iv[0]+1) >= min_points]\n",
    "    if len(intervals)==0:\n",
    "        intervals = [(s,e)]\n",
    "    return intervals\n",
    "\n",
    "def split_all_strokes(df, pen_strokes, prom=0.05, dist=8, min_points=6):\n",
    "    subs = []\n",
    "    for (s,e) in pen_strokes:\n",
    "        parts = split_stroke_by_speed_minima(df, s, e, prom=prom, dist=dist, min_points=min_points)\n",
    "        subs.extend(parts)\n",
    "    return subs\n",
    "\n",
    "# Beta & ellipse helpers\n",
    "def beta_velocity(t, A, t0, t1, a, b):\n",
    "    t = np.array(t)\n",
    "    v = np.zeros_like(t, dtype=float)\n",
    "    denom = (t1 - t0) if (t1 - t0) != 0 else 1e-8\n",
    "    u = (t - t0) / denom\n",
    "    mask = (u > 0) & (u < 1)\n",
    "    uu = u[mask]\n",
    "    with np.errstate(all='ignore'):\n",
    "        v_mask = A * (uu**a) * ((1 - uu)**b)\n",
    "    v[mask] = v_mask\n",
    "    return v\n",
    "\n",
    "def fit_beta_to_substroke(df, s, e, min_points=8):\n",
    "    seg = df.iloc[s:e+1].reset_index(drop=True)\n",
    "    if len(seg) < min_points:\n",
    "        return None\n",
    "    t = seg['time_s'].to_numpy()\n",
    "    speed = seg['speed'].to_numpy()\n",
    "    A0 = max(np.max(speed), 1e-3)\n",
    "    t0_0 = t[0]\n",
    "    t1_0 = t[-1]\n",
    "    p0 = [A0, t0_0, t1_0, 2.0, 2.0]\n",
    "    bounds = ([0.0, t[0]-0.5, t[-1]-1.0, 0.05, 0.05],\n",
    "              [A0*20+1.0, t[0]+0.5, t[-1]+0.5, 12.0, 12.0])\n",
    "    try:\n",
    "        popt, _ = curve_fit(beta_velocity, t, speed, p0=p0, bounds=bounds, maxfev=20000)\n",
    "        return {\"A\":float(popt[0]), \"t0\":float(popt[1]), \"t1\":float(popt[2]), \"a\":float(popt[3]), \"b\":float(popt[4])}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fit_ellipse_least_squares(xs, ys):\n",
    "    x = xs[:,None]; y = ys[:,None]\n",
    "    D = np.hstack([x*x, x*y, y*y, x, y, np.ones_like(x)])\n",
    "    try:\n",
    "        U, S, Vt = np.linalg.svd(D, full_matrices=False)\n",
    "        a = Vt.T[:,-1]\n",
    "        A, B, Cc, Dd, E, Ff = a.flatten()\n",
    "    except Exception:\n",
    "        return None\n",
    "    denom = B*B - 4*A*Cc\n",
    "    if np.isclose(denom, 0):\n",
    "        return None\n",
    "    x0 = (2*Cc*Dd - B*E) / denom\n",
    "    y0 = (2*A*E - B*Dd) / denom\n",
    "    up = 2*(A*E*E + Cc*Dd*Dd + Ff*B*B - 2*B*Dd*E - A*Cc*Ff)\n",
    "    tmp = (A - Cc)**2 + B*B\n",
    "    sqrt_tmp = np.sqrt(np.abs(tmp))\n",
    "    down1 = (denom) * ((Cc - A) + sqrt_tmp)\n",
    "    down2 = (denom) * ((Cc - A) - sqrt_tmp)\n",
    "    if down1 == 0 or down2 == 0:\n",
    "        return None\n",
    "    try:\n",
    "        a_len = np.sqrt(np.abs(up / down1))\n",
    "        b_len = np.sqrt(np.abs(up / down2))\n",
    "    except Exception:\n",
    "        return None\n",
    "    if b_len > a_len:\n",
    "        a_len, b_len = b_len, a_len\n",
    "    theta = 0.5 * np.arctan2(B, (A - Cc))\n",
    "    ecc = np.sqrt(max(0, 1 - (b_len**2)/(a_len**2))) if a_len!=0 else 0.0\n",
    "    return {\"xc\":float(x0), \"yc\":float(y0), \"a\":float(a_len), \"b\":float(b_len), \"angle_rad\":float(theta), \"ecc\":float(ecc)}\n",
    "\n",
    "print(\"Helper functions loaded — ready to run the sequence builder (Cell 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbdcd69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'META_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm   \u001b[38;5;66;03m# plain tqdm (no notebook mode)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# --- Load metadata ---\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m meta \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[43mMETA_FILE\u001b[49m)\n\u001b[0;32m     10\u001b[0m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     11\u001b[0m meta_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m], meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease\u001b[39m\u001b[38;5;124m'\u001b[39m]))  \u001b[38;5;66;03m# \"PD\" or \"H\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'META_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2 — generate sequences for ALL .svc files (BLSTM-ready)\n",
    "import json, gc, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm   # plain tqdm (no notebook mode)\n",
    "\n",
    "# --- Load metadata ---\n",
    "meta = pd.read_excel(META_FILE)\n",
    "meta['ID'] = meta['ID'].astype(str).str.zfill(5)\n",
    "meta_map = dict(zip(meta['ID'], meta['Disease']))  # \"PD\" or \"H\"\n",
    "\n",
    "# --- List files ---\n",
    "svc_paths = sorted(PAHAW_PUBLIC.rglob(\"*.svc\"))\n",
    "print(\"Total svc files:\", len(svc_paths))\n",
    "\n",
    "# --- Storage ---\n",
    "all_seqs = []     # list of (n_substrokes, feature_dim) arrays\n",
    "all_labels = []   # PD=1, H=0 per file\n",
    "all_meta = []     # store {\"path\",\"subject\"}\n",
    "failed = []\n",
    "\n",
    "# --- Resume feature ---\n",
    "partial_file = PROCESSED_DIR / \"all_sequences_partial.pkl\"\n",
    "\n",
    "if partial_file.exists():\n",
    "    print(\"Resuming from partial file...\")\n",
    "    with open(partial_file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    all_seqs = data.get(\"seqs\", [])\n",
    "    all_labels = data.get(\"labels\", [])\n",
    "    all_meta = data.get(\"meta\", [])\n",
    "    processed_paths = set(data.get(\"paths\", []))\n",
    "else:\n",
    "    processed_paths = set()\n",
    "\n",
    "print(\"Already processed:\", len(processed_paths))\n",
    "\n",
    "# --- Feature vector for a single substroke ---\n",
    "def compute_substroke_feature_vector(seg, df_local, s_abs, e_abs):\n",
    "    duration = seg['time_s'].iloc[-1] - seg['time_s'].iloc[0]\n",
    "    amp_x = seg['x_s'].max() - seg['x_s'].min()\n",
    "    amp_y = seg['y_s'].max() - seg['y_s'].min()\n",
    "    amplitude = math.sqrt(amp_x**2 + amp_y**2)\n",
    "    mean_speed = seg['speed'].mean()\n",
    "    mean_press = seg['press'].mean()\n",
    "\n",
    "    # beta & ellipse\n",
    "    beta = fit_beta_to_substroke(df_local, s_abs, e_abs)\n",
    "    ell  = fit_ellipse_least_squares(seg['x_s'].to_numpy(),\n",
    "                                     seg['y_s'].to_numpy())\n",
    "\n",
    "    beta_A = beta['A'] if isinstance(beta, dict) else np.nan\n",
    "    beta_a = beta['a'] if isinstance(beta, dict) else np.nan\n",
    "    beta_b = beta['b'] if isinstance(beta, dict) else np.nan\n",
    "\n",
    "    ell_a  = ell['a'] if isinstance(ell, dict) else np.nan\n",
    "    ell_b  = ell['b'] if isinstance(ell, dict) else np.nan\n",
    "    ell_ecc = ell['ecc'] if isinstance(ell, dict) else np.nan\n",
    "\n",
    "    # simple fuzzy: compare against global medians\n",
    "    f_speed_high = 1.0 if mean_speed > np.nanmedian(df_local['speed']) else 0.0\n",
    "    f_press_high = 1.0 if mean_press > np.nanmedian(df_local['press']) else 0.0\n",
    "    f_curv_high = 1.0 if seg['curvature'].mean() > np.nanmedian(df_local['curvature']) else 0.0\n",
    "\n",
    "    return np.array([\n",
    "        duration, amplitude, mean_speed, mean_press,\n",
    "        beta_A, beta_a, beta_b,\n",
    "        ell_a, ell_b, ell_ecc,\n",
    "        f_speed_high, f_press_high, f_curv_high\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "new_files = 0\n",
    "\n",
    "for p in tqdm(svc_paths, desc=\"Processing SVC files\"):\n",
    "    pstr = str(p)\n",
    "\n",
    "    # skip already processed\n",
    "    if pstr in processed_paths:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_local = load_svc(p)\n",
    "        if len(df_local) < 5:\n",
    "            failed.append({\"path\": pstr, \"reason\": \"too short\"})\n",
    "            processed_paths.add(pstr)\n",
    "            continue\n",
    "\n",
    "        df_local = preprocess_df(df_local)\n",
    "        pen_strokes = segment_by_pen(df_local)\n",
    "        subs = split_all_strokes(df_local, pen_strokes, prom=0.05, dist=8, min_points=6)\n",
    "\n",
    "        feats = []\n",
    "        for (s, e) in subs:\n",
    "            seg = df_local.iloc[s:e+1].reset_index(drop=True)\n",
    "            if len(seg) < 6:\n",
    "                continue\n",
    "            vec = compute_substroke_feature_vector(seg, df_local, s, e)\n",
    "            feats.append(vec)\n",
    "\n",
    "        if len(feats) == 0:\n",
    "            failed.append({\"path\": pstr, \"reason\": \"no substrokes\"})\n",
    "            processed_paths.add(pstr)\n",
    "            continue\n",
    "\n",
    "        seq_arr = np.vstack(feats)\n",
    "        all_seqs.append(seq_arr)\n",
    "\n",
    "        # label based on subject ID\n",
    "        subject = p.parent.name\n",
    "        label = 1 if meta_map.get(subject, \"H\") == \"PD\" else 0\n",
    "        all_labels.append(label)\n",
    "\n",
    "        all_meta.append({\"path\": pstr, \"subject\": subject})\n",
    "\n",
    "        processed_paths.add(pstr)\n",
    "        new_files += 1\n",
    "\n",
    "        # periodic saving every 25 files\n",
    "        if new_files % 25 == 0:\n",
    "            with open(partial_file, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"seqs\": all_seqs,\n",
    "                    \"labels\": all_labels,\n",
    "                    \"meta\": all_meta,\n",
    "                    \"paths\": list(processed_paths)\n",
    "                }, f)\n",
    "            print(f\"\\nSaved partial progress after {new_files} new files.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append({\"path\": pstr, \"reason\": str(e)})\n",
    "        processed_paths.add(pstr)\n",
    "        continue\n",
    "\n",
    "\n",
    "# --- Final save ---\n",
    "np.savez(PROCESSED_DIR / \"all_sequences.npz\",\n",
    "         sequences=all_seqs,\n",
    "         labels=all_labels,\n",
    "         meta=all_meta)\n",
    "\n",
    "with open(PROCESSED_DIR / \"all_sequences_failed.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"failed\": failed}, f)\n",
    "\n",
    "print(\"\\nFinished!\")\n",
    "print(\"Total files attempted:\", len(svc_paths))\n",
    "print(\"Total sequences:\", len(all_seqs))\n",
    "print(\"Failed:\", len(failed))\n",
    "print(\"Saved to:\", PROCESSED_DIR / \"all_sequences.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
